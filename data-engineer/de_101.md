# Data Engineering 101
- Data ~ "Flow"
- Upstream (Data producers) --> data manipulated (Data Engineer) --> Downstream (Data consumers)
- Example of Business Analytics (BI)
  - Data sources: website (customer and order data), stripe (payment data), salesforce (crm, sales data)
  - Data engineering: store, organize data, transform
  - Data analysts: dashboard
- Example of Real-time (Doordash)
  - Delivery person: pick up the order --> real-time GPS coordinates
  - Data engineering: event processing and streaming system (Kafka, Apache Flink) --> real-time processed events
  - Mobile app: display delivery status
- Why is DE important?
  - Explosion of data sources, data uses
  - New Apps --> more data
  - Smart Devices
  - Reliable Data --> Good Decisions
- End to end data pipeline
  - Generation --> Ingestion --> Transformation --> Serving --> Analytics, ML/AI, Reverse ETL
      <br>--> Storage (Data Engineering)
  - Orchestration, Security, Data Governance, DataOps, Data Quality, Software Engineering
- Historical Context:
  - 1980-2000: Data warehouses
    - DW + Scalable Analytics (BI)
    - MPP: Massively Parallel Processing
    - BI engineer, ETL developer, DW engineer
    - On-prem (vs Cloud), monolithic data stores
  - 2000s: The beginning of "Big Data"
    - Internet + Google, Amazon, etc.
    - The birth of the Cloud (economical, commodity hardware)
    - AWS: S3, EC2
    - Google: paper on MapReduce - Hadoop
  - 2000s-2010s: Big Data Engineering
    - Big Data ~ Hadoop
    - Hadoop Ecosystem: Hadoop, YARN, HDFS
    - A lot of work to manage
    - Requires a specialized engineering team
  - 2020s: Modern Data Stack
    - Collection of open sources, third party tools that are connected.
    - Tools: dbt core, flyte, iceberg, hudi, feast, great expectations,trino, dagster, etc.
    - NO single tech stack that makes up the entire pipeline.
    - Hiding way the details of low-level concerns that plagued Big Data era technologies like Hadoop
- Responsibilities: business and technical
  - Communicate with both technical and non-technical people
  - Understand how to scope and manage a data project
  - Minimize cost and work within budget
  - Create a good data architecture
  - Build and manage a reliable and performant data pipelines
  - Security, data governance, automation, observability, etc.
  - Technical skills: SQL, Python (Java, Scala), Basic DevOps, Bash

## End to end Data Pipeline
- Types of Databases
  - Relational database (RDBMS)
    - Used to store transactional data
    - Normalized
  - Non-relational database (NoSQL)
    - De-normalized
    - Data is less structured
  - Key-value store
    - Caching application data
    - Fast lookup and high concurrency
    - Simple and single table
- Third-party systems via API
  - REST API, GraphQL, gRPC
- Do NOT consider the source systems as someone else's problem
  - Upstream source system --> Downstream data pipeline
  - Validation and testing
  - Communicate with your data producers
- Storage formats - Serialization
  - Turning data into byte streams to easily save or transport it. 
  - Data --> Serializer --byte stream--> File, Data store, Memory
  - pandas --> Serializer --> Parquet
  - Serialize the data into a standard format which is sent around and deserialized on the receiving end.
  - Row-based serialization: XML, JSON, CSV
    - Fast lookup of individual rows
  - Column-based serialization: Parquet, ORC, Avro, Arrow
    - Aggregation by column
- Compression:
  - Reduce the amount of data stored
  - Faster query (less to search)
  - Faster transport (less data to move)
- Caching:
  - Some storages are faster to access
  - Memory is 1000x faster than SSDs and CPU caches are orders of magnitude faster than memory
  - Faster storage is more limited (smaller) and expensive